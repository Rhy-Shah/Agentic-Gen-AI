{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0a41ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d55408f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AgenticAI'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "babbf00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "## This value has to be set to true always\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "660e20be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x156f929e0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x156f92ea0> root_client=<openai.OpenAI object at 0x156f90180> root_async_client=<openai.AsyncOpenAI object at 0x156f92b10> model_name='o1-mini' temperature=1.0 model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"o1-mini\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfda1833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of India is **New Delhi**.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 85, 'prompt_tokens': 14, 'total_tokens': 99, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o1-mini-2024-09-12', 'system_fingerprint': 'fp_3da8b0b088', 'id': 'chatcmpl-BcmmMLCz9In5AYnDo439MFfbrQmlE', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--af5b0413-5bb6-4db3-8b37-1ef5dce65351-0', usage_metadata={'input_tokens': 14, 'output_tokens': 85, 'total_tokens': 99, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm.invoke(\"What is the capital of India?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af8900f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of India is **New Delhi**.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61cd5353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"qwen-qwq-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0600e1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n<think>\\nOkay, the user said \"Hi, I am Rhythm.\" I need to respond in a friendly way. Let me start by greeting them back. Maybe say \"Hello, Rhythm!\" to acknowledge their name. Then, I should offer assistance. I can ask how I can help them today. Keep it simple and open-ended so they feel comfortable to ask anything. Let me check if there\\'s anything else I should consider. No, that\\'s probably enough. Alright, let\\'s put it together.\\n\\nWait, maybe add an emoji to make it more welcoming? Like a smiley face? The user might appreciate a friendly tone. Yeah, that\\'s good. So the response would be: \"Hello, Rhythm! ðŸ˜Š How can I assist you today?\" That sounds good. Short, friendly, and invites them to state their request. I think that\\'s all.\\n</think>\\n\\nHello, Rhythm! ðŸ˜Š How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 193, 'prompt_tokens': 16, 'total_tokens': 209, 'completion_time': 0.474020867, 'prompt_time': 0.003197878, 'queue_time': 0.016852449999999998, 'total_time': 0.477218745}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_512a3da6bb', 'finish_reason': 'stop', 'logprobs': None}, id='run--cae5b5a5-ff9a-459c-9d33-399a50015ebc-0', usage_metadata={'input_tokens': 16, 'output_tokens': 193, 'total_tokens': 209})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hi, I am Rhythm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640bcb39",
   "metadata": {},
   "source": [
    "### PROMPT ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "448afac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI assistant. Provide me with concise and accurate answers.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt =  ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI assistant. Provide me with concise and accurate answers.\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e599d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
