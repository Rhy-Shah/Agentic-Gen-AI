{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f0a41ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d55408f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AgenticAI'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "babbf00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "## This value has to be set to true always\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "660e20be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x112ccc270> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x112ccc6b0> root_client=<openai.OpenAI object at 0x110163df0> root_async_client=<openai.AsyncOpenAI object at 0x112ccc380> model_name='o1-mini' temperature=1.0 model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model = \"o1-mini\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cfda1833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of India is **New Delhi**.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 405, 'prompt_tokens': 14, 'total_tokens': 419, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 384, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o1-mini-2024-09-12', 'system_fingerprint': 'fp_3da8b0b088', 'id': 'chatcmpl-BcnYl0VmfvoyA3feHMYPX1Ikyza3G', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--3b974d13-60de-465c-9e42-3d94a174dfaa-0', usage_metadata={'input_tokens': 14, 'output_tokens': 405, 'total_tokens': 419, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 384}})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.invoke(\"What is the capital of India?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "af8900f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of India is **New Delhi**.'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "61cd5353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"qwen-qwq-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0600e1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n<think>\\nOkay, the user introduced themselves as Rhythm. I should respond politely and maybe ask how I can assist them today. Let me keep it friendly and open-ended so they feel comfortable to ask for help with whatever they need. Maybe something like, \"Hi Rhythm! How can I assist you today?\" That should work.\\n\\nWait, maybe I should add a bit more to make it welcoming. Maybe mention that I\\'m here to help with any questions or tasks. Let me check the guidelines again to make sure I\\'m following the right tone. Yep, friendly and helpful. Alright, the response should be concise but inviting. Let me go with that.\\n\\n</think>\\n\\nHi Rhythm! Nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific! ðŸ˜Š', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 171, 'prompt_tokens': 16, 'total_tokens': 187, 'completion_time': 0.390734962, 'prompt_time': 0.002900875, 'queue_time': 0.056769559000000004, 'total_time': 0.393635837}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_a91d9c2cfb', 'finish_reason': 'stop', 'logprobs': None}, id='run--d1c38fc6-5d44-45a7-a211-3d6e082bc8da-0', usage_metadata={'input_tokens': 16, 'output_tokens': 171, 'total_tokens': 187})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hi, I am Rhythm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640bcb39",
   "metadata": {},
   "source": [
    "### PROMPT ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "448afac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI assistant. Provide me with concise and accurate answers.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt =  ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI assistant. Provide me with concise and accurate answers.\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1e599d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x1134e27b0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x1134e2430>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"gemma2-9b-it\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6b017894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI assistant. Provide me with concise and accurate answers.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x1134e27b0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x1134e2430>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model\n",
    "\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "93a04e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Eiffel Tower was built in 1887-1889 as the entrance arch for the 1889 World's Fair in Paris, celebrating the centennial of the French Revolution. \\n\\nDesigned by Gustave Eiffel and his company, it was initially met with strong opposition from artists and intellectuals who considered it an eyesore. However, it quickly became a beloved symbol of Paris and a global icon. \\n\\nThe tower was initially meant to be dismantled after 20 years but was saved due to its use for radio transmissions.  Today, it's one of the most visited monuments in the world. \\n\\n\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"input\": \"Can you tell me about the history of the Eiffel Tower?\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c36d4fb",
   "metadata": {},
   "source": [
    "### Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2af09fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bb3e560b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"input\": \"What is the capital of France?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "df7f28d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(template = \"Answer the user query \\n {format_instructions} \\n {input} \\n\", input_variables = [\"query\"], \n",
    "                        partial_variables = {\"format_instructions\": output_parser.get_format_instructions()})\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8ea2f96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Bitcoin is a decentralized digital currency, without a central bank or single administrator, that can be sent from user to user on the peer-to-peer bitcoin network without the need for intermediaries. Transactions are verified by network nodes through cryptography and recorded in a public distributed ledger called a blockchain.', 'key_features': ['Decentralized:', 'Digital:', 'Peer-to-peer:', 'Cryptographically secure:', 'Transparent and immutable ledger (blockchain):', 'Limited supply (21 million bitcoins)'], 'impact_on_finance': ['Alternative to traditional currencies:', 'Potential for faster and cheaper transactions:', 'New investment asset class:', 'Innovation in financial technology'], 'risks': ['Volatility:', 'Security risks:', 'Regulatory uncertainty:', 'Lack of consumer protection']}\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"input\": \"What is Bitcoin in the world of Finance?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d417900",
   "metadata": {},
   "source": [
    "### Using ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "79d80ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant that provides concise and accurate answers. Provide the answer in json format.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that provides concise and accurate answers. Provide the answer in json format.\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b16fa65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'definition': 'Financial instruments whose value is derived from an underlying asset, such as a stock, bond, commodity, or currency.', 'characteristics': [\"Value fluctuates based on the underlying asset's performance.\", 'Used for hedging, speculation, or arbitrage.', 'Examples include options, futures, swaps, and forwards.']}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | model | output_parser\n",
    "response = chain.invoke({\"input\": \"What are Derivatives in Finance?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f56173",
   "metadata": {},
   "source": [
    "### XML Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "66958888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant that provides concise and accurate answers. Provide the answer in XML format.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that provides concise and accurate answers. Provide the answer in XML format.\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "355733db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "defusedxml is not installed. Please install it to use the defusedxml parser.You can install it with `pip install defusedxml`See https://github.com/tiran/defusedxml for more details",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/langchain_core/output_parsers/xml.py:210\u001b[39m, in \u001b[36mXMLOutputParser.parse\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdefusedxml\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ElementTree\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'defusedxml'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[116]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m chain = prompt | model | output_parser\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response = chain.invoke({\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWhat are Derivatives in Finance?\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/langchain_core/runnables/base.py:3047\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3045\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3046\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3047\u001b[39m                 input_ = context.run(step.invoke, input_, config)\n\u001b[32m   3048\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3049\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/langchain_core/output_parsers/base.py:196\u001b[39m, in \u001b[36mBaseOutputParser.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    190\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    193\u001b[39m     **kwargs: Any,\n\u001b[32m    194\u001b[39m ) -> T:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    197\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result(\n\u001b[32m    198\u001b[39m                 [ChatGeneration(message=inner_input)]\n\u001b[32m    199\u001b[39m             ),\n\u001b[32m    200\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    201\u001b[39m             config,\n\u001b[32m    202\u001b[39m             run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    203\u001b[39m         )\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    206\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    207\u001b[39m         config,\n\u001b[32m    208\u001b[39m         run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    209\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/langchain_core/runnables/base.py:1940\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1937\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1938\u001b[39m         output = cast(\n\u001b[32m   1939\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m             context.run(\n\u001b[32m   1941\u001b[39m                 call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   1942\u001b[39m                 func,\n\u001b[32m   1943\u001b[39m                 input_,\n\u001b[32m   1944\u001b[39m                 config,\n\u001b[32m   1945\u001b[39m                 run_manager,\n\u001b[32m   1946\u001b[39m                 **kwargs,\n\u001b[32m   1947\u001b[39m             ),\n\u001b[32m   1948\u001b[39m         )\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1950\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/langchain_core/runnables/config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/langchain_core/output_parsers/base.py:197\u001b[39m, in \u001b[36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[39m\u001b[34m(inner_input)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    190\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    193\u001b[39m     **kwargs: Any,\n\u001b[32m    194\u001b[39m ) -> T:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m    196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result(\n\u001b[32m    198\u001b[39m                 [ChatGeneration(message=inner_input)]\n\u001b[32m    199\u001b[39m             ),\n\u001b[32m    200\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    201\u001b[39m             config,\n\u001b[32m    202\u001b[39m             run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    203\u001b[39m         )\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    206\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    207\u001b[39m         config,\n\u001b[32m    208\u001b[39m         run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    209\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/langchain_core/output_parsers/base.py:250\u001b[39m, in \u001b[36mBaseOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, result: \u001b[38;5;28mlist\u001b[39m[Generation], *, partial: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> T:\n\u001b[32m    236\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse a list of candidate model Generations into a specific format.\u001b[39;00m\n\u001b[32m    237\u001b[39m \n\u001b[32m    238\u001b[39m \u001b[33;03m    The return value is parsed from only the first Generation in the result, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    248\u001b[39m \u001b[33;03m        Structured output.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parse(result[\u001b[32m0\u001b[39m].text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/langchain_core/output_parsers/xml.py:218\u001b[39m, in \u001b[36mXMLOutputParser.parse\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    212\u001b[39m         msg = (\n\u001b[32m    213\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdefusedxml is not installed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    214\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease install it to use the defusedxml parser.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    215\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou can install it with `pip install defusedxml`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    216\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/tiran/defusedxml for more details\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    217\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    219\u001b[39m     _et = ElementTree  \u001b[38;5;66;03m# Use the defusedxml parser\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mImportError\u001b[39m: defusedxml is not installed. Please install it to use the defusedxml parser.You can install it with `pip install defusedxml`See https://github.com/tiran/defusedxml for more details"
     ]
    }
   ],
   "source": [
    "chain = prompt | model | output_parser\n",
    "response = chain.invoke({\"input\": \"What are Derivatives in Finance?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfa0277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
