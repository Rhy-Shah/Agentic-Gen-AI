{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2073d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from langgraph.types import Command\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.tools import tool\n",
    "from typing_extensions import Literal\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac255335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm just a virtual assistant, so I can't feel emotions, but I'm here and ready to help you with anything you need. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 14, 'total_tokens': 51, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bnu99SVn99zwdbiZOJHuCmtOfGK6n', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--7d1cbc0c-c30f-44f8-b6a3-c1c0b0bd77a3-0', usage_metadata={'input_tokens': 14, 'output_tokens': 37, 'total_tokens': 51, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm.invoke(\"Hey there, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2f5a1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_number(state):\n",
    "    result = state[\"num1\"] + state[\"num2\"]\n",
    "    print(f\"addition is {result}\")\n",
    "    \n",
    "    return Command(goto = \"multiply\", update = {\"sum\": result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a27eabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addition is 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Command(update={'sum': 15}, goto='multiply')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = {\"num1\": 5, \"num2\": 10}\n",
    "add_number(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f67796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def transfer_to_addition_expert():\n",
    "    \"\"\"Ask addition agent for help\"\"\"\n",
    "    return\n",
    "\n",
    "@tool\n",
    "def transfer_to_multiplication_expert():\n",
    "    \"\"\"Ask multiplication agent for help\"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b00b4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tool = llm.bind_tools([transfer_to_multiplication_expert, transfer_to_addition_expert])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79ca6bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm_with_tool.invoke(\"What is 5 + 10?\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf5eb44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'transfer_to_addition_expert',\n",
       "  'args': {},\n",
       "  'id': 'call_SXooM1oFtiJ5CvbKZyyFJSRD',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3f7ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\"You are an addition expert, you can ask the multiplication expert for help with multiplication\"\n",
    "                 \"Always do your portion of calculation first, then ask the multiplication expert if needed\")\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [\"Can you add 5 and 10?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c7bdc55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an addition expert, you can ask the multiplication expert for help with multiplicationAlways do your portion of calculation first, then ask the multiplication expert if needed'},\n",
       " 'Can you add 5 and 10?']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "368c5087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addition_agent(state: MessagesState) -> Command[Literal[\"multiplication_agent\", \"__end__\"]]:\n",
    "    system_prompt = \"You are an addition expert, and you can ask for help from the multiplication expert when asked to multiply numbers.Always do your portion of calculation first, and then ask the multiplication expert if needed.\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    \n",
    "    ai_message = llm.bind_tools([transfer_to_multiplication_expert]).invoke(messages)\n",
    "    \n",
    "    if len(ai_message.tool_calls) > 0:\n",
    "        tool_call_id = ai_message.tool_calls[-1][\"id\"]\n",
    "        tool_message = {\"role\": \"tool\", \"content\": \"Successfully transferred to multiplication expert.\", \"tool_call_id\": tool_call_id}\n",
    "\n",
    "        return Command(goto=\"multiplication_agent\", update={\"messages\": [ai_message, tool_message]})\n",
    "\n",
    "    return {\"messages\":[ai_message]}\n",
    "\n",
    "\n",
    "def multiplication_agent(state: MessagesState) -> Command[Literal[\"addition_agent\", \"__end__\"]]:\n",
    "    system_prompt = \"You are a multiplication expert, and you can ask for help from the addition expert when asked to add numbers.Always do your portion of calculation first, and then ask the addition expert if needed.\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "\n",
    "    ai_message = llm.bind_tools([transfer_to_addition_expert]).invoke(messages)\n",
    "\n",
    "    if len(ai_message.tool_calls) > 0:\n",
    "        tool_call_id = ai_message.tool_calls[-1][\"id\"]\n",
    "        tool_message = {\"role\": \"tool\", \"content\": \"Successfully transferred to addition expert.\", \"tool_call_id\": tool_call_id}\n",
    "\n",
    "        return Command(goto=\"addition_agent\", update={\"messages\": [ai_message, tool_message]})\n",
    "\n",
    "    return {\"messages\":[ai_message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "516415da",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(MessagesState)\n",
    "\n",
    "graph.add_node(\"addition_agent\", addition_agent)\n",
    "graph.add_node(\"multiplication_agent\", multiplication_agent)\n",
    "\n",
    "graph.add_edge(START, \"addition_agent\")\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8e6ddb30",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_G2IteNX9KAxogecLxQY6QS5n\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m app.invoke({\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [(\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mWhat is (3 + 5) * 12\u001b[39m\u001b[33m\"\u001b[39m)]})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/langgraph/pregel/__init__.py:2719\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2716\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2717\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2719\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream(\n\u001b[32m   2720\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2721\u001b[39m     config,\n\u001b[32m   2722\u001b[39m     stream_mode=stream_mode,\n\u001b[32m   2723\u001b[39m     output_keys=output_keys,\n\u001b[32m   2724\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   2725\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   2726\u001b[39m     checkpoint_during=checkpoint_during,\n\u001b[32m   2727\u001b[39m     debug=debug,\n\u001b[32m   2728\u001b[39m     **kwargs,\n\u001b[32m   2729\u001b[39m ):\n\u001b[32m   2730\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2731\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2732\u001b[39m             \u001b[38;5;28misinstance\u001b[39m(chunk, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m   2733\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m (ints := chunk.get(INTERRUPT)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2734\u001b[39m         ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/langgraph/pregel/__init__.py:2436\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2434\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2435\u001b[39m             loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.tick(\n\u001b[32m   2437\u001b[39m             [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2438\u001b[39m             timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2439\u001b[39m             get_waiter=get_waiter,\n\u001b[32m   2440\u001b[39m             schedule_task=loop.accept_push,\n\u001b[32m   2441\u001b[39m         ):\n\u001b[32m   2442\u001b[39m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2443\u001b[39m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[32m   2444\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36maddition_agent\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      2\u001b[39m system_prompt = \u001b[33m\"\u001b[39m\u001b[33mYou are an addition expert, and you can ask for help from the multiplication expert when asked to multiply numbers.Always do your portion of calculation first, and then ask the multiplication expert if needed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m messages = [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: system_prompt}] + state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m ai_message = llm.bind_tools([transfer_to_multiplication_expert]).invoke(messages)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ai_message.tool_calls) > \u001b[32m0\u001b[39m:\n\u001b[32m      8\u001b[39m     tool_call_id = ai_message.tool_calls[-\u001b[32m1\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/langchain_core/runnables/base.py:5431\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5424\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5425\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5426\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5429\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5430\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5431\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.invoke(\n\u001b[32m   5432\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5433\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5434\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5435\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28mself\u001b[39m.generate_prompt(\n\u001b[32m    373\u001b[39m             [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    374\u001b[39m             stop=stop,\n\u001b[32m    375\u001b[39m             callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    376\u001b[39m             tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    377\u001b[39m             metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    378\u001b[39m             run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    379\u001b[39m             run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    380\u001b[39m             **kwargs,\n\u001b[32m    381\u001b[39m         ).generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28mself\u001b[39m._generate_with_cache(\n\u001b[32m    777\u001b[39m                 m,\n\u001b[32m    778\u001b[39m                 stop=stop,\n\u001b[32m    779\u001b[39m                 run_manager=run_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    780\u001b[39m                 **kwargs,\n\u001b[32m    781\u001b[39m             )\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(\n\u001b[32m   1023\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1024\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:973\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    971\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m    972\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m     response = \u001b[38;5;28mself\u001b[39m.client.create(**payload)\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m    926\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    927\u001b[39m         body=maybe_transform(\n\u001b[32m    928\u001b[39m             {\n\u001b[32m    929\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m    930\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m    931\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m    932\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m    933\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m    934\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m    935\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m    936\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m    937\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m    938\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m    939\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    940\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m    941\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m    942\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m    943\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m    944\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m    945\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m    946\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m    947\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m    948\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m    949\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m    950\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m    951\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m    952\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m    953\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m    954\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m    955\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m    956\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m    957\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m    958\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m    959\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m    960\u001b[39m             },\n\u001b[32m    961\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m    962\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m    963\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m    964\u001b[39m         ),\n\u001b[32m    965\u001b[39m         options=make_request_options(\n\u001b[32m    966\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m    967\u001b[39m         ),\n\u001b[32m    968\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m    969\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    970\u001b[39m         stream_cls=Stream[ChatCompletionChunk],\n\u001b[32m    971\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/openai/_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/openai/_base_client.py:1034\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1031\u001b[39m             err.response.read()\n\u001b[32m   1033\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_G2IteNX9KAxogecLxQY6QS5n\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}",
      "During task with name 'addition_agent' and id '0b3a36fc-f54c-ac35-33fc-4b041c915ed5'"
     ]
    }
   ],
   "source": [
    "app.invoke({\"messages\": [(\"user\", \"What is (3 + 5) * 12\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2b5e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9ca005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "61942646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from typing import Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2d35b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAVILY_API_KEY=os.getenv(\"TAVILY_API_KEY\")\n",
    "search_tool=TavilySearchResults(tavily_api_key=TAVILY_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f506569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'List of prime ministers of the United Kingdom - Simple English ...',\n",
       "  'url': 'https://simple.wikipedia.org/wiki/List_of_Prime_Ministers_of_the_United_Kingdom',\n",
       "  'content': 'Wikipedia\\nThe Free Encyclopedia\\n\\n## Contents\\n\\n# List of prime ministers of the United Kingdom\\n\\nThe Prime Minister of the United Kingdom is the leader of His Majesty\\'s Government. They chair Cabinet meetings. It is the highest civil office in the United Kingdom.[source?] The present prime minister has been Keir Starmer since 5 July 2024.\\n\\n## The procedure [...] | Name | Time in office | Political party |\\n| --- | --- | --- |\\n| Keir Starmer | 2024 - present | Labour \"Labour Party (UK)\") |\\n| Rishi Sunak | 2022 – 2024 | Conservative \"Conservative Party (UK)\") |\\n| Liz Truss | 2022 | Conservative |\\n| Boris Johnson | 2019 – 2022 | Conservative |\\n| Theresa May | 2016 – 2019 | Conservative |\\n| David Cameron | 2010 – 2016 | Conservative |\\n| Gordon Brown | 2007 – 2010 | Labour |\\n| Tony Blair | 1997 – 2007 | Labour |\\n| John Major | 1990 – 1997 | Conservative |',\n",
       "  'score': 0.8573986},\n",
       " {'title': 'Prime Minister - GOV.UK',\n",
       "  'url': 'https://www.gov.uk/government/ministers/prime-minister',\n",
       "  'content': 'The Prime Minister is the leader of His Majesty’s Government and is ultimately responsible for the policy and decisions of the government.\\n\\nAs leader of the UK government the Prime Minister also:\\n\\n## Current role holder\\n\\n### The Rt Hon Sir Keir Starmer KCB KC MP\\n\\nSir Keir Starmer became Prime Minister on 5 July 2024.\\n\\n## Education',\n",
       "  'score': 0.8235701},\n",
       " {'title': 'Prime Minister of the United Kingdom - Wikipedia',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Prime_Minister_of_the_United_Kingdom',\n",
       "  'content': \"Prime Minister of the United Kingdom ; Incumbent Keir Starmer. since 5 July 2024 ; Government of the United Kingdom · Prime Minister's Office\",\n",
       "  'score': 0.8051581},\n",
       " {'title': 'Latest Keir Starmer Approval Ratings 2025 - politicalpulse.net',\n",
       "  'url': 'https://politicalpulse.net/uk-polls/keir-starmer-approval-rating/',\n",
       "  'content': \"Latest Keir Starmer Approval Ratings 2025: Keir Starmer's approval rating is at its lowest, marking a great decline within a very short time since the May 2024 elections. Keir Starmer is the Labour Party leader and the current Prime Minister of the United Kingdom.\",\n",
       "  'score': 0.8005209},\n",
       " {'title': 'Keir Starmer elected new UK prime minister after big Labour Party win',\n",
       "  'url': 'https://www.foxnews.com/world/keir-starmer-become-new-british-prime-minister-after-big-labour-party-win',\n",
       "  'content': \"Keir Starmer officially became the new prime minister of the United Kingdom after the Labor Party's big win Thursday in the general election, The Associated Press reported.\",\n",
       "  'score': 0.78014404}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_tool.invoke(\"who is a current pm of uk?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f4633ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: SyntaxWarning: invalid escape sequence '\\`'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\`'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\`'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\`'\n",
      "/var/folders/c4/kvf3th_j15lf5cq2pybvfgjm0000gn/T/ipykernel_40528/4192633359.py:15: SyntaxWarning: invalid escape sequence '\\`'\n",
      "  result_str = f\"Successfully executed:\\n\\`\\`\\`python\\n{code}\\n\\`\\`\\`\\nStdout: {result}\"\n",
      "/var/folders/c4/kvf3th_j15lf5cq2pybvfgjm0000gn/T/ipykernel_40528/4192633359.py:15: SyntaxWarning: invalid escape sequence '\\`'\n",
      "  result_str = f\"Successfully executed:\\n\\`\\`\\`python\\n{code}\\n\\`\\`\\`\\nStdout: {result}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='python_repl_tool', description='Use this to execute python code. If you want to see the output of a value,\\nyou should print it out with `print(...)`. This is visible to the user.', args_schema=<class 'langchain_core.utils.pydantic.python_repl_tool'>, func=<function python_repl_tool at 0x133dda520>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repl=PythonREPL()\n",
    "\n",
    "@tool\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    \n",
    "    result_str = f\"Successfully executed:\\n\\`\\`\\`python\\n{code}\\n\\`\\`\\`\\nStdout: {result}\"\n",
    "    return (\n",
    "        result_str + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )\n",
    "    \n",
    "python_repl_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b3e5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_system_prompt(instruction:str)->str:\n",
    "    return  (\n",
    "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "        \" Use the provided tools to progress towards answering the question.\"\n",
    "        \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "        \" will help where you left off. Execute what you can to make progress.\"\n",
    "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "        \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "        f\"\\n{instruction}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "60394dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are a helpful AI assistant, collaborating with other assistants. Use the provided tools to progress towards answering the question. If you are unable to fully answer, that's OK, another assistant with different tools  will help where you left off. Execute what you can to make progress. If you or any of the other assistants have the final answer or deliverable, prefix your response with FINAL ANSWER so the team knows to stop.\\nYou can only do research. You are working with a chart generator colleague.\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_system_prompt(\"You can only do research. You are working with a chart generator colleague.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cec22c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_node(last_message:BaseMessage, goto:str):\n",
    "    if \"FINAL ANSWER\" in last_message.content:\n",
    "        # Any agent decided the work is done\n",
    "        return END\n",
    "    return goto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3869f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_node(state:MessagesState)->Command[Literal[\"chart_generator\", END]]:\n",
    "    research_agent=create_react_agent(\n",
    "        llm,\n",
    "        tools=[search_tool],\n",
    "        prompt=make_system_prompt(\n",
    "        \"You can only do research. You are working with a chart generator colleague.\"\n",
    "    ), \n",
    "        )\n",
    "    \n",
    "    result=research_agent.invoke(state)\n",
    "    result = result[\"messages\"][-1]\n",
    "    \n",
    "    # result=[messages:{humanmesssage\n",
    "    #                 aimessgae\n",
    "    #                 toolmessage\n",
    "    #                 aimessage\n",
    "    #                 toolmessage\n",
    "    #                 aimessage}]\n",
    "    \n",
    "    \n",
    "    goto=get_next_node(result[\"messages\"][-1],\"chart_generator\")\n",
    "    \n",
    "    result[\"messages\"][-1] = HumanMessage(content=result[\"messages\"][-1].content, name=\"researcher\")\n",
    "    \n",
    "    return Command(update={\"messages\": result[\"messages\"]},goto=goto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4310531e",
   "metadata": {},
   "source": [
    "agent1(human1)-->agent2(human2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2dd18e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chart_node(state:MessagesState)-> Command[Literal[\"researcher\", END]]:\n",
    "    chart_agent=create_react_agent(\n",
    "        llm,\n",
    "        tools=[python_repl_tool],\n",
    "        prompt=make_system_prompt(\n",
    "        \"You can only generate charts. You are working with a researcher colleague.\"\n",
    "    ),\n",
    "        )\n",
    "    result=chart_agent.invoke(state)\n",
    "    goto=get_next_node(result[\"messages\"][-1],\"researcher\")\n",
    "    result[\"messages\"][-1] = HumanMessage(content=result[\"messages\"][-1].content, name=\"chart_generator\")\n",
    "    return Command(update={\"messages\": result[\"messages\"]},goto=goto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "abe5ffb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x133c5c6b0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"researcher\", research_node)\n",
    "workflow.add_node(\"chart_generator\", chart_node)\n",
    "\n",
    "workflow.add_edge(START, \"researcher\")\n",
    "app = workflow.compile()\n",
    "\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eaff8a0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'AIMessage' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m app.invoke({\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [(\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mget the UK\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms GDP over the past 3 years, then make a line chart of it.Once you make the chart, finish.\u001b[39m\u001b[33m\"\u001b[39m)],})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/langgraph/pregel/__init__.py:2719\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2716\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2717\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2719\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream(\n\u001b[32m   2720\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2721\u001b[39m     config,\n\u001b[32m   2722\u001b[39m     stream_mode=stream_mode,\n\u001b[32m   2723\u001b[39m     output_keys=output_keys,\n\u001b[32m   2724\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   2725\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   2726\u001b[39m     checkpoint_during=checkpoint_during,\n\u001b[32m   2727\u001b[39m     debug=debug,\n\u001b[32m   2728\u001b[39m     **kwargs,\n\u001b[32m   2729\u001b[39m ):\n\u001b[32m   2730\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2731\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2732\u001b[39m             \u001b[38;5;28misinstance\u001b[39m(chunk, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m   2733\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m (ints := chunk.get(INTERRUPT)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2734\u001b[39m         ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/langgraph/pregel/__init__.py:2436\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2434\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2435\u001b[39m             loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.tick(\n\u001b[32m   2437\u001b[39m             [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2438\u001b[39m             timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2439\u001b[39m             get_waiter=get_waiter,\n\u001b[32m   2440\u001b[39m             schedule_task=loop.accept_push,\n\u001b[32m   2441\u001b[39m         ):\n\u001b[32m   2442\u001b[39m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2443\u001b[39m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[32m   2444\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mresearch_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     11\u001b[39m result = result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m]\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# result=[messages:{humanmesssage\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#                 aimessgae\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#                 toolmessage\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#                 aimessage\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m#                 toolmessage\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m#                 aimessage}]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m goto=get_next_node(result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m],\u001b[33m\"\u001b[39m\u001b[33mchart_generator\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m] = HumanMessage(content=result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].content, name=\u001b[33m\"\u001b[39m\u001b[33mresearcher\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Command(update={\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]},goto=goto)\n",
      "\u001b[31mTypeError\u001b[39m: 'AIMessage' object is not subscriptable",
      "During task with name 'researcher' and id '7508da5d-3c73-aed2-cf13-42c5a27a4144'"
     ]
    }
   ],
   "source": [
    "app.invoke({\"messages\": [(\"user\",\"get the UK's GDP over the past 3 years, then make a line chart of it.Once you make the chart, finish.\")],})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb4f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
