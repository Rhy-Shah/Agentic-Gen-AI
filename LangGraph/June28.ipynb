{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37e822b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from langgraph.types import Command\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.tools import tool\n",
    "from typing_extensions import Literal\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from typing import Annotated, Literal, List, Dict, Any\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "664aeeef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here to help. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 14, 'total_tokens': 44, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BoLXmIgYn6Sb6KyKWGUgbRTynNIHY', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--1ac9bf13-ff5c-4df5-8ca6-78f999bdfa8b-0', usage_metadata={'input_tokens': 14, 'output_tokens': 30, 'total_tokens': 44, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm.invoke(\"Hey there, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5598b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "search_tool = TavilySearchResults(tavily_api_key=TAVILY_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "241a78e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "members=[\"researcher\",\"coder\"]\n",
    "options = members+[\"FINISH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f62a7992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router(TypedDict):\n",
    "    next: Literal[\"researcher\", \"coder\", \"FINISH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd5f474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    next:str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "221ec256",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\"\n",
    "You are a supervisor, tasked with managing a conversation between the following workers: {members}. \n",
    "Given the following user request, respond with the worker to act next. \n",
    "Each worker will perform a task and respond with their results and status. \n",
    "When finished, respond with FINISH.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca79308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor(state:State)->Command[Literal['researcher', 'coder', '__end__']]:\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},] + state[\"messages\"]\n",
    "    \n",
    "    llm_with_structure_output = llm.with_structured_output(Router)\n",
    "    response = llm_with_structure_output.invoke(messages)\n",
    "    goto=response[\"next\"]\n",
    "    \n",
    "    print(\"**********BELOW IS MY GOTO***************\")    \n",
    "    print(goto)\n",
    "    \n",
    "    if goto == \"FINISH\":\n",
    "        goto=END\n",
    "    \n",
    "    return Command(goto=goto, update={\"next\":goto})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07c54af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def researcher(state: State) -> Command[Literal['supervisor', 'coder', 'FINISH']]:\n",
    "    \n",
    "    research = create_react_agent(llm, tools=[search_tool], prompt = \"You are just a researcher, tasked with finding information based on the user request. Do not do any math\")\n",
    "    result = research.invoke(state)\n",
    "\n",
    "    return Command(update = {\"messages\":[HumanMessage(content=result[\"messages\"][-1].content, name=\"researcher\")]}, goto=\"supervisor\")\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38daa67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coder(state: State) -> Command[Literal['supervisor', 'coder', 'FINISH']]:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85fbc97d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found edge ending at unknown node `FINISH`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m graph.add_node(\u001b[33m'\u001b[39m\u001b[33mcoder\u001b[39m\u001b[33m'\u001b[39m, coder)\n\u001b[32m      7\u001b[39m graph.add_edge(START, \u001b[33m'\u001b[39m\u001b[33msupervisor\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m app = graph.compile()\n\u001b[32m     11\u001b[39m app\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/langgraph/graph/state.py:612\u001b[39m, in \u001b[36mStateGraph.compile\u001b[39m\u001b[34m(self, checkpointer, cache, store, interrupt_before, interrupt_after, debug, name)\u001b[39m\n\u001b[32m    609\u001b[39m interrupt_after = interrupt_after \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m    611\u001b[39m \u001b[38;5;66;03m# validate the graph\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m \u001b[38;5;28mself\u001b[39m.validate(\n\u001b[32m    613\u001b[39m     interrupt=(\n\u001b[32m    614\u001b[39m         (interrupt_before \u001b[38;5;28;01mif\u001b[39;00m interrupt_before != \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m []) + interrupt_after\n\u001b[32m    615\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m interrupt_after != \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    616\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    617\u001b[39m     )\n\u001b[32m    618\u001b[39m )\n\u001b[32m    620\u001b[39m \u001b[38;5;66;03m# prepare output channels\u001b[39;00m\n\u001b[32m    621\u001b[39m output_channels = (\n\u001b[32m    622\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__root__\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.schemas[\u001b[38;5;28mself\u001b[39m.output]) == \u001b[32m1\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m     ]\n\u001b[32m    630\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ArizonaStateUniversity/SelfStudy/Agentic & Gen AI with Cloud/agenticai/lib/python3.13/site-packages/langgraph/graph/graph.py:304\u001b[39m, in \u001b[36mGraph.validate\u001b[39m\u001b[34m(self, interrupt)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m all_targets:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nodes \u001b[38;5;129;01mand\u001b[39;00m target != END:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound edge ending at unknown node `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    305\u001b[39m \u001b[38;5;66;03m# validate interrupts\u001b[39;00m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupt:\n",
      "\u001b[31mValueError\u001b[39m: Found edge ending at unknown node `FINISH`"
     ]
    }
   ],
   "source": [
    "graph = StateGraph(MessagesState)\n",
    "\n",
    "graph.add_node('supervisor', supervisor)\n",
    "graph.add_node('researcher', researcher)\n",
    "graph.add_node('coder', coder)\n",
    "\n",
    "graph.add_edge(START, 'supervisor')\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853449a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
